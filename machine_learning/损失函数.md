



### 0-1损失、绝对损失、平方损失



### 对数损失、指数损失



### hinge损失



### 交叉熵

**a) KL散度(相对熵)**

​        对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。

​         维基百科中的定义：即如果用P来描述目标问题，而不是用Q来描述目标问题，得到的信息增量。

​        在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1] 
$$
D_{KL}(p||q) =  \sum_i^n p(x_i)*log(\frac {p(x_i)} {q(x_i)})  \tag 1
$$
**b) 交叉熵**

​     对公式(1)变形后有
$$
D_{KL}(p||q) =  \sum_{i=1}^n p(x_i)*log({p(x_i)} -  \sum_{i=1}^n p(x_i)*log({q(x_i)} \\
 =-H(p(x)) + [-\sum_{i=1}^n p(x_i)*log({q(x_i)} ]
$$
​    等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：
$$
H(p,q) = -\sum_{i=1}^n p(x_i)*log({q(x_i)}  \tag 2
$$


​    在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即$D_{KL}(p||q)$，由于KL散度中的前一部分$-H(p(x))$不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。



参考：<a href=https://blog.csdn.net/tsyccnh/article/details/79163834>交叉熵在机器学习中的使用</a>