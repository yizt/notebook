



参考：[标量、向量、矩阵导数](https://blog.csdn.net/u010976453/article/details/54381248)

​        [反向传导算法](http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)

​        [softmax loss求导](https://blog.csdn.net/u014313009/article/details/51045303)

$\left( \begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13}&amp;a_{14} \\&nbsp;a_{21}&amp;a_{22}&amp;a_{23}&amp;a_{24} \\&nbsp;a_{31}&amp;a_{32}&amp;a_{33}&amp;a_{34} \\<br>a_{41}&amp;a_{42}&amp;a_{43}&amp;a_{44}&nbsp;\end{array} \right) * \left( \begin{array}{ccc}<br>\delta_{11}&amp; \delta_{12} \\ \delta_{21}&amp;\delta_{22}&nbsp;\end{array} \right)$



[卷积反向传播](http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/)



[Forward And Backpropagation in Convolutional Neural Network](https://medium.com/@2017csm1006/forward-and-backpropagation-in-convolutional-neural-network-4dfa96d7b37e)



[Convnet: Implementing Convolution Layer with Numpy](https://wiseodd.github.io/techblog/2016/07/16/convnet-conv-layer/)



a⊗b=a(1-b)

[刘建平Pinard](https://www.cnblogs.com/pinard/p/6494810.html) 

https://grzegorzgwardys.wordpress.com/2016/04/22/8/





 第$l$ 层的高度坐标为i的神经元接收的是l-1层坐标范围为$[i,i+k_1^{l-1}-1]$ 的神经元; 反过来对于第l-1层高度坐标$i^{\prime}$ ,$[i^{\prime},i+k_1^{l-1}-1]$、$[i^{\prime}-1,i+k_1^{l-1}-2]$ .. $[i^{\prime}-k_1^{l-1}+1,i^{\prime}]$ 这几个坐标范围包含了$i^{\prime}$ , 对应的范围是$i \in [i^{\prime}-k_1^{l-1}+1,i^{\prime}]$ , 根据公式(4)中的约束条件还要保证$i \in [0,H^{l-1} - k_1^{l-1}]$ ; 宽度坐标$j$ 相同的推导方法;因此公式(5)为
$$
\begin{align}
\delta^{l-1}_{i^{\prime},j^{\prime}}
=\frac {\partial L} {\partial z_{i^{\prime},j^{\prime}}^{l-1}} 
=\sum_{i= \max (0,i^{\prime}-k_1^{l-1}+1)}^{\min(i^{\prime},H^{l-1}-k^{l-1}_1)}
	\sum_{j= \max (0,j^{\prime}-k_2^{l-1}+1)}^{\min(i^{\prime},\hat W^{l-1}-k^{l-1}_2)}
    	\delta^l_{i,j} * z^{l-1}_{i+m,j+n} \tag 6
\end{align} \\
$$
