[TOC]





## ML

### xgboost

#### 特性

正则化

二阶导数近似

尺寸收缩(类似学习率)

列下采样

近似算法(贪婪学习)

带权百分比摘要(Weighted Quantile Sketch)

稀疏模型处理



#### 系统实现方法

列快并行(column block) 预计算：近似时多个块(每个块包含部分行)，对local proposal algorithms；每个列的统计可以并行

Cache-aware获取

堆外计算(块压缩和分片) 



> 优点 

- XGB利用了二阶梯度来对节点进行划分，相对其他GBM来说，精度更加高。
- 利用局部近似算法对分裂节点的贪心算法优化，取适当的eps时，可以保持算法的性能且提高算法的运算速度。
- 在损失函数中加入了L1/L2项，控制模型的复杂度，提高模型的鲁棒性。
- 提供并行计算能力，主要是在树节点求不同的候选的分裂点的Gain Infomation（分裂后，损失函数的差值）
- Tree Shrinkage，column subsampling等不同的处理细节。

> 缺点

- 需要pre-sorted，这个会耗掉很多的内存空间（2 * #data * # features）
- 数据分割点上，由于XGB对不同的数据特征使用pre-sorted算法而不同特征其排序顺序是不同的，所以分裂时需要对每个特征单独做依次分割，遍历次数为#data * #features来将数据分裂到左右子节点上。
- 尽管使用了局部近似计算，但是处理粒度还是太细了
- 由于pre-sorted处理数据，在寻找特征分裂点时（level-wise），会产生大量的cache随机访问。

> 因此LightGBM针对这些缺点进行了相应的改进。

1. LightGBM基于histogram算法代替pre-sorted所构建的数据结构，利用histogram后，会有很多有用的tricks。例如histogram做差，提高了cache命中率（主要是因为使用了leaf-wise）。
2. 在机器学习当中，我们面对大数据量时候都会使用采样的方式（根据样本权值）来提高训练速度。又或者在训练的时候赋予样本权值来关于于某一类样本（如Adaboost）。LightGBM利用了GOSS来做采样算法。
3. 由于histogram算法对稀疏数据的处理时间复杂度没有pre-sorted好。因为histogram并不管特征值是否为0。因此我们采用了EFB来预处理稀疏数据。



#### 参考：

https://zhuanlan.zhihu.com/p/38516467









## 基础网络

### Network In Network



参考：https://www.cnblogs.com/yinheyi/p/6978223.html



### ResNet v1

#### 1. 介绍

网络深度非常重要；

深度导致难以训练已经较大程度解决:  Normalized 初始化、  Batch Normalization

深度带来另一个问题：网络退化，并非过拟合(训练和测试误差都增大)

更深的网络不应该比更浅的网络差(假定最后几层就学习一个恒等映射)；说明学习恒等映射很难

考虑学习一个残差(极致情况，权重全为0就可以了)；而且不会增加计算量和参数。

实验结果表明：更容易优化、性能随深度单调增加

#### 3. 深度残差学习

存在一个假说:多层非线性函数可以逼近复杂的函数; 那么同样可以逼近我们的残差

如果恒等映射是最优的，那么残差网络将权重学习为0就可以了

实验表明恒等映射通常有很小的反应

**网络结构**

a)相同的feature map大小,相同的channel, b) feature map 减半，深度加倍

c) 维度变化使用一个1*1的卷积来解决



#### 4. 实验

普通深度网络深度加深到一定程度后训练和验证误差都增加，**推测**是由于**收敛速率指数级降低** 

恒等映射 vs 投影映射 ；后者效果稍好，但会增加参数

Bottleneck架构：层数增加，参数没有增加

残差网络的相应更小

1202层和152层训练误差一样，但验证误差更高



### ResNet V2

​          探索传播方式，前向和后向信号可以直接从一个block传到另一个block。

#### 介绍

​         聚焦直接传递信息，不仅在残差单元内，而是整个网络；

​         预激活思想设计一个全新的残差单元



#### 分析残差网络

​        如果xl+1 ≡ yl; 信息可以传递到任何浅层单元

#### Skip Connections的影响

​       shortcut connections是信息传递的最直接路径



#### 激活分析

​      预激活有两个优点：容易优化、防止过拟合





## 目标检测

### R-CNN

​          PASCAL VOC目标检测进入停滞期, mAP 58.3; 两个见解: CNN用到region proposal中来定位对象；监督预训练和领域精调。

#### 简介

​        进展缓慢； 2012 AlexNet 在ImageNet分类引起关注，CNN分类任务多大程度能够泛化到目标检测。

​        关注两个问题：CNN定位对象，小量标注，大容量模型。

​        解决方法：region proposal和监督预训练(以前是无监督预训练)；



#### R-CNN目标检测

​         三个模块 region proposal，cnn, svm分类

​         高效：特征少，共享计算

**训练过程**

​        监督预训练

​        领域精调: 1:3 正负样本比，



### SSD



低分辨率获取高精度，进一步促进速度提升

不同feature maps预测不同的尺寸

使用小的卷积过滤器到feature maps上



## 语义分割

RefineNet、PSPNet

### DeepLab v1

#### 摘要

​        三个贡献: 

a)空洞卷积:控制分辨率、增大感受野、保持参数不变

b) ASPP-空洞金字塔池化: 鲁棒分割多尺寸对象，多重采样比例和视野，来捕捉不同尺寸的对象

c) 组合DCNN和PGM: 提升边界定位，卷积网络的下采样影响定位精度，通过组合DCNN最后一层响应和全连接CRF，提升定位性能



#### 简介

​        DCNN在视觉识别任务(图像分类、目标检测)取得很大成功，关键的因素是DCNN对于局部图像变换的内在不变性；可以学习数据的抽象表示，对于分类很好；但会妨碍像语义分割这种密集预测任务，抽象的空间信息在这里不合适。

​         DCNN遇到的三个挑战: 分辨率的降低、目标的多尺寸、	不变性带来的定位精度降低

第一个挑战: 去除最后几个池化层，使用滤波器上采样(就是空洞卷积)；组合多个空洞卷积，接一个双线性插值到原图分辨率。

第二个挑战：不同于多尺寸输入，使用多个并行的不同采样率的空洞卷积，称之为ASPP.

第三个挑战：一种方式使用skip-layers，最终用多层来预测最终分割结果。我们使用全连接CRF，捕获边缘细节，满足长距离依赖

​          DeepLab的三个优点：

a)速度：8FPS, 全连接CRF 0.5秒   b) 

b)精度：VOC 2012 79.7%

c) 简单：级联两个固定的模块DCNN和CRF





#### 相关工作

​         像素分类，完全抛弃分割；组合DCNN和局部CRF，将superpixs作为顶点对待，忽略远程依赖。我们的方法将像素作为顶点，捕获远程依赖，服从快速平均场推断。

​        其它论文的重要和有价值的

a) 端到端的训练，结构化预测：我们的CRF是后处理步骤，已经有人提出的端到端联合训练 ；使用一个卷积层近似密集CRF平均场推断的一个迭代；另一个方向是使用DCNN学习CRF的成对项 

b) 弱监督：无需整张图都有像素级别标注



#### 方法

##### 空洞卷积抽取特征和增大视野

​        一种修复分辨率减小的方法是使用反卷积，这需要额外的内存和耗时。我们使用空洞卷积，可以应用到任意层，满足任意分辨率，并无缝集成。

​         可以在任意高分辨率上计算DCNN最终的响应；例如：为了加倍特征响应的空间密度；将最后一个池化层的补充设置为1，接下来所有的卷积层的采样比设置为2。在所有层上使用这种方法，可以获得原始图像分辨率的响应，但是这计算量太大。采用混合方式，平滑效率/精度;最后两层使用空洞卷积, 然后8倍双线性插值，恢复到原图分辨率。双线性插值不需要学习任何参数，速度更快

​         空洞卷积可以在任意层，任意增大视野，计算和参数保持不变。大的采样率，对性能有提升。

##### ASPP表示多尺寸图像

​         DCNN有隐式代表图像尺寸的能力，明确说明对象尺寸可以提高DCNN成功处理大尺寸和小尺寸对象的能力。

​         第一种是多尺寸处理。

​         第二种是重采样单尺寸的卷积层特征；我们使用ASPP，泛化了DeepLab-LargeFOV。



##### 全连接CRF结构化预测

​         DCNN顶层节点只有很平滑的响应，只能预测对象的粗略位置；以前有两种方式来处理：一是多层预测，而是采用超像素表示，使用低级分割方法。

​         我们组合DCNN和全连接CRF，传统的CRF用于弱分类器的平滑，相邻像素倾向于相同类别。DCNN的特征图原本就很平滑，我们的目标不是平滑而是恢复局部结构的细节

​         第一项依赖像素位置和RGB颜色，第二项只依赖像素。第一项迫使相似颜色和位置的像素有相同的标签；第二项仅考虑像素位置。



#### 实验结果

​         ImageNet预训练模型、交叉熵损失函数、在8倍下采样的每个空间计算损失、DCNN和CRF分开训练。先介绍会议版本，接下来是最近结果



##### PASCAL VOC 2012

​         20个目标对象，一个背景类; 1, 464 (train)、1, 449 (val), and 1, 456 (test) 像素级别标注图像；性能度量是21个类别上像素级别IoU

**会议版本结果**

​         使用VGG16预训练模型，mini-batch 20，学习率0.001，每2000个迭代学习率缩减10倍，权重衰减0.0005、动量大小0.9。

​          最终使用3*3的卷积核、大的采样率12、fc6和fc7层神经元改为1000个，形成DeepLab-LargeFOV版本。CRF可以提升3~5个百分点。



**会议版本后的改进**

a)学习率策略

​        使用poly学习率，比sgd好1.17%

b)采用ASPP

​        多个并行的fc6-fc7-fc8分支，fc6 是3*3卷积，ASPP-S和 ASPP-L的采样率分别为{2, 4, 8, 12}和{6, 12, 18, 24}

CRF前ASPP-S比LargeFOV好1.22%；经过CRF后基本一样(也就是ASPP没有效果)。ASPP-L还是有提升的。

c)更深的网络和多尺寸处理

​      使用ResNet101; 分别输入scale = {0.5, 0.75,1}；在score map层融合最大响应；在MS-COCO上预训练，训练时随机缩放0.5~1.5; 最终79.7%的精度。

​     

##### PASCAL-Context

​          语义标注了整个图像包括对象和stuff；在最常见的59类+背景类上评估；训练数据4998；验证集5105；最好精度为45.7%



##### PASCAL-Person-Part

​       包含更多的尺寸和姿态，标注了人体的每一部分；融合标注为Head, Torso,Upper/Lower Arms and Upper/Lower Legs 6类+1个背景类；1716个训练集，1817个验证集。最好精度为64.94%。

​        但是LargeFOV or ASPP在这个数据集上没有作用。



##### Cityscapes

​       高质量的像素级别标注，来自50个城市的5000张街景图像。19个类别属于7个大类： ground, construction,object, nature, sky, human, and vehicle；

​        training, validation, and test 分别为 2975, 500, 1525 ；最好精度71.4。

​        图像原始分辨率为2048×1024；没有使用多尺寸。



##### 失败案例

​       不能捕获纤细物体的边界，如自行车、椅子



### Deeplab V2





### 总结

​        空洞卷积、ASPP、组合DCNN和全连接CRF。





参考：https://blog.csdn.net/u013580397/article/details/78508392

[DeepLab(1,2,3)系列总结](https://blog.csdn.net/u011974639/article/details/79148719)

[DeepLab v3+](https://www.paperweekly.site/papers/notes/326)

## 人脸识别

### Docface+

#### DWI-AMSoftmax Loss

$$
w_j= \frac {w_j^*}  {||w_j^*||_2}  \tag 4
$$

$$
w_j^*=(1-\alpha)w_j + \alpha w_j^{batch}  \tag 5
$$

$w_j^{batch}$ 是根据当前mini-batch计算出的目标权重向量; 就是嵌入特征 $f$  的L2归一化值；注意$w_j$ 也只更新当前mini-batch中的权重。

margin m 为5.0,最初使用



Face-ResNet架构

一般的方法到ID-自拍数据集上迁移效果不好：收敛慢，陷入局部极小值；

由于数据集浅，造成欠拟合

**数据采样** 

​           随机采样$B/2$ 各类别；$B$ 是batch-size; 然后从每个类别各采集一个ID图像和自拍图像





### 总结

https://www.jianshu.com/p/1dd8c0364710



## OCR/场景文本检测、识别



### CRNN

类序列识别，长度变化大；CNN不能可变

end-to-end、不需要字符级别标注、不受限于固定词典、序列长度

### CTPN

固定宽度、垂直的anchor

CNN+RNN组合,in-network rnn

end-to-end

#### 简介

之前的检测器不鲁棒、不可靠；使用低级特征，区别单个笔画或字符，没有使用上下文信息。

faster r-cnn很在目标检测上成功;但无法直接用到这里，因为需要更高的定位精度，这是细粒度的识别

定位准、in-network rnn、适用多尺寸、多语言、end-to-end

#### 相关工作

Connected-componets:快速过滤器区分像素文本/非文本; 然后使用低级特征分组为笔画或字符候选

sliding-window:多尺寸密集滑窗检测字符候选; 计算量大

这两种通用方法受限于字符检测性能

#### Connectionist Text Proposal Network

细粒度的提议框、recurrent connectionist text proposals、side-refinement

细粒度的提议框：text line可以较好的区分文本，固定宽度,预测垂直的效果更好；anchor垂直数量10,范围11~273; 同时预测评分和垂直的边框位置;细粒度的的检测提供了更加精细的监督信息，导致更精准的定位

Recurrent Connectionist Text Proposals: 独立的检测proposals，导致类文本的噪声;上下文信息保障可靠

Side-Refinement: 水平距离小于50像素(垂直IoU>0.7)的proposals连起来; 精调两侧的边框水平方向位置







### 总结

https://blog.csdn.net/xwukefr2tnh4/article/details/80589198



## 其它

视觉会议基本参考

PAMI/IJCV/JMLR 4 分；

TIP/TNNLS/TCSVT 2 分；

CVPR/ICCV/ECCV/NIPS/ICML 1.5 分

PR/CVIU/PRL/neurocomputing/AAAI/IJCAI 1 分